@article{VO2023105999,
title = {Toward complete coverage planning using deep reinforcement learning by trapezoid-based transformable robot},
journal = {Engineering Applications of Artificial Intelligence},
volume = {122},
pages = {105999},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.105999},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623001835},
author = {Dinh Tung Vo and Anh Vu Le and Tri Duc Ta and Minh Tran and Phan Van Duc and Minh Bui Vu and Nguyen Huu Khanh Nhan},
keywords = {Transformable robotics, Reconfigurable tiling robotics, Cleaning and maintenance, Complete coverage planning, Deep reinforcement learning},
abstract = {Shape-shifting robots are the feasible solutions to solve the Complete Coverage Planning (CCP) problem. These robots can extend the covered areas by reconfiguring their shape to different forms according to space conditions. Since energy usage while navigating is constrained by the number of shape-shifting, it is desirable to cover the confined areas by trajectory using minimal robot actions within finite states. This paper presents a CCP method using deep reinforcement learning (DRL) for a reconfigurable robot with a trapezoid shape called Transbot. The framework derives optimal action policy for robot trajectory within the grid-based workspace. DRL model relies on Convolutional Neural Networks (CNNs) with Long Short Temporary Memory (LSTM) layers using Actor-Critic with Experience Replay (ACER) as the decision layers. The trained DRL model simultaneously generates robot shapes and directions with optimal energy cost by maximizing the cumulative reward representing the Transbot actions. By creating the trajectory with less 28.95% energy and 19.42% time in tested simulation and real-world experiments, the proposed CCP framework outperforms the existing tiling-based heuristic optimization techniques.}
}
@article{CHANG2023102570,
title = {Hierarchical multi-robot navigation and formation in unknown environments via deep reinforcement learning and distributed optimization},
journal = {Robotics and Computer-Integrated Manufacturing},
volume = {83},
pages = {102570},
year = {2023},
issn = {0736-5845},
doi = {https://doi.org/10.1016/j.rcim.2023.102570},
url = {https://www.sciencedirect.com/science/article/pii/S0736584523000467},
author = {Lu Chang and Liang Shan and Weilong Zhang and Yuewei Dai},
keywords = {Multi-robot systems (MRSs), Deep reinforcement learning, Mobile robot navigation, Collision avoidance, Formation control, Distributed optimization},
abstract = {Compared with a single robot, Multi-robot Systems (MRSs) can undertake more challenging tasks in complex scenarios benefiting from the increased transportation capacity and fault tolerance. This paper presents a hierarchical framework for multi-robot navigation and formation in unknown environments with static and dynamic obstacles, where the robots compute and maintain the optimized formation while making progress to the target together. In the proposed framework, each single robot is capable of navigating to the global target in unknown environments based on its local perception, and only limited communication among robots is required to obtain the optimal formation. Accordingly, three modules are included in this framework. Firstly, we design a learning network based on Deep Deterministic Policy Gradient (DDPG) to address the global navigation task for single robot, which derives end-to-end policies that map the robot’s local perception into its velocity commands. To handle complex obstacle distributions (e.g. narrow/zigzag passage and local minimum) and stabilize the training process, strategies of Curriculum Learning (CL) and Reward Shaping (RS) are combined. Secondly, for an expected formation, its real-time configuration is optimized by a distributed optimization. This configuration considers surrounding obstacles and current formation status, and provides each robot with its formation target. Finally, a velocity adjustment method considering the robot kinematics is designed which adjusts the navigation velocity of each robot according to its formation target, making all the robots navigate to their targets while maintaining the expected formation. This framework allows for formation online reconfiguration and is scalable with the number of robots. Extensive simulations and 3-D evaluations verify that our method can navigate the MRS in unknown environments while maintaining the optimal formation.}
}
@article{ZHANG20209465,
title = {Robot Navigation among External Autonomous Agents through Deep Reinforcement Learning using Graph Attention Network},
journal = {IFAC-PapersOnLine},
volume = {53},
number = {2},
pages = {9465-9470},
year = {2020},
note = {21st IFAC World Congress},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2020.12.2419},
url = {https://www.sciencedirect.com/science/article/pii/S2405896320331013},
author = {Tianle Zhang and Tenghai Qiu and Zhiqiang Pu and Zhen Liu and Jianqiang Yi},
keywords = {Robot navigation, deep reinforcement learning (DRL), graph attention network},
abstract = {Finding collision-free and efficient paths in an uncertain dynamic environment is a challenge for robot navigation tasks, especially when there are external autonomous agents that also have decision-making abilities in the same environment. This paper develops a novel method based on DRL with graph attention network (GAT) to solve the problem of robot navigation among external autonomous agents (other agents). Specifically, GAT is adopted to describe the robot and other agents as a specific graph, and extract the spatial structural influence features of other agents on the robot from the graph. Multi-head attention mechanism is utilized to calculate the weights of interactions between the robot and other agents. This GAT uses observations of an arbitrary number of other agents in dynamic environments. Furthermore, the proposed method is combined with optimal reciprocal collision avoidance to improve its safety in new environments. Various simulations demonstrate that our method has good performance and robustness in different environments.}
}
@article{KONISHI202216,
title = {Efficient Safe Control via Deep Reinforcement Learning and Supervisory Control – Case Study on Multi-Robot Warehouse Automation},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {28},
pages = {16-21},
year = {2022},
note = {16th IFAC Workshop on Discrete Event Systems WODES 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.10.318},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322023552},
author = {Masahiro Konishi and Tomotake Sasaki and Kai Cai},
keywords = {Safe Control, Supervisory Control, Deep Reinforcement Learning},
abstract = {Safe control has recently attracted much attention due to its applications in safety-critical cyber-physical systems. Supervisory control theory (SCT) is a formal control method that provides correct-by-construction safety certificates, but is computationally inefficient when the number of system components is large. On the other hand, deep reinforcement learning (DRL) provides a toolbox of efficient algorithms to compute control decisions even for very large state space, but does not always guarantee safety. In this paper, we propose to synergize SCT and DRL into a new efficient safe control approach. Specifically, we first employ DRL algorithms to efficiently compute sub-optimal solutions which may be unsafe; then we convert the obtained solutions into a standard supervisory control problem with an automaton (plant model) and a set of unsafe states (safety specification); finally we use SCT to synthesize a supervisor with a safety certificate. A case study of multi-robot warehouse logistic automation is conducted to demonstrate the efficiency of this proposed approach.}
}
@article{XING2022102918,
title = {Robot path planner based on deep reinforcement learning and the seeker optimization algorithm},
journal = {Mechatronics},
volume = {88},
pages = {102918},
year = {2022},
issn = {0957-4158},
doi = {https://doi.org/10.1016/j.mechatronics.2022.102918},
url = {https://www.sciencedirect.com/science/article/pii/S0957415822001362},
author = {Xiangrui Xing and Hongwei Ding and Zhuguan Liang and Bo Li and Zhijun Yang},
keywords = {Seeker optimization algorithm, Advantage actor-critic, Path planning, Path de-redundancy, Deep reinforcement learning},
abstract = {Path planning is one of the key technologies for mobile robot applications. However, the traditional robot path planner has a slow planning response, which leads to a long navigation completion time. In this paper, we propose a novel robot path planner (SOA+A2C) that produces global and local path planners with the seeker optimization algorithm (SOA) and the advantage actor-critic (A2C) algorithm, respectively. In addition, to solve the problems of poor convergence performance when training deep reinforcement learning (DRL) agents in complex path planning tasks and path redundancy when metaheuristic algorithms, such as SOA, are used for path planning, we propose the incremental map training method and path de-redundancy method. Simulation results show that first, the incremental map training method can improve the convergence performance of the DRL agent in complex path planning tasks. Second, the path de-redundancy method can effectively alleviate path redundancy without sacrificing the search capability of the metaheuristic algorithm. Third, the SOA+A2C path planner is superior to the Dijkstra & dynamic window approach (Dijkstra+DWA) and the Dijkstra & timed elastic band (Dijkstra+TEB) path planners provided by the robot operating system (ROS) in terms of path length, path planning response time, and navigation completion time. Therefore, the developed SOA+A2C path planner can serve as an effective tool for mobile robot path planning.}
}
@article{SIVASHANGARAN2021218,
title = {Intelligent Autonomous Navigation of Car-Like Unmanned Ground Vehicle via Deep Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {54},
number = {20},
pages = {218-225},
year = {2021},
note = {Modeling, Estimation and Control Conference MECC 2021},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2021.11.178},
url = {https://www.sciencedirect.com/science/article/pii/S2405896321022229},
author = {Shathushan Sivashangaran and Minghui Zheng},
keywords = {Actor-Critic Network, Autonomous Vehicles, Deep Deterministic Policy Gradient, Deep Reinforcement Learning, Intelligent Navigation, Motion Planning},
abstract = {In this paper, a car-like Unmanned Ground Vehicle (UGV) is simulated and trained as an intelligent agent to navigate and exit unknown obstacle filled environments given no prior knowledge of environment characteristics, using a Reinforcement Learning (RL) algorithm tailored for continuous action spaces. This is achieved using Deep Deterministic Policy Gradient (DDPG), an Actor-Critic network that combines multiple cutting-edge Artificial Intelligence methods including continuous Deep-Q learning, policy gradient methods and actor-critic networks. A combination of two feedforward neural networks with Rectified Linear Units (ReLU) is used for the critic and actor representations which combine both policy and value based methods to learn continuous action space policies via approximation functions. The role of the actor network in this architecture is to decide linear and angular velocity outputs from a continuous action space given current state inputs, to then be evaluated by the critic network to learn and estimate Q-values by minimizing a loss function. The proposed DDPG RL network is trained and evaluated in two obstacle filled environments for a car-like UGV with wheelbase, l of 0.3 m. During the 10,000 episode training period, the agent converges to a maximum reward value of 180 after 1100 training episodes in the first environment, and a maximum reward value of 80 after 7500 training episodes in the second, more complex environment. The agent is shown to exhibit intelligent human-like learning behavior to learn optimal policies and adapt to new environments at the end of each training period with no changes to network architecture.}
}
@article{QU2023114016,
title = {Pursuit-evasion game strategy of USV based on deep reinforcement learning in complex multi-obstacle environment},
journal = {Ocean Engineering},
volume = {273},
pages = {114016},
year = {2023},
issn = {0029-8018},
doi = {https://doi.org/10.1016/j.oceaneng.2023.114016},
url = {https://www.sciencedirect.com/science/article/pii/S0029801823004006},
author = {Xiuqing Qu and Wenhao Gan and Dalei Song and Liqin Zhou},
keywords = {Unmanned surface vehicles, Pursuit-evasion game, Deep reinforcement learning, Imitation learning, Obstacle avoidance},
abstract = {Aiming at the confrontation game problems between pursuit-evasion unmanned surface vehicles under complex multi-obstacle environment, a pursuit-evasion game strategy is proposed. Firstly, the multi-obstacle environment is set up, and the gaming situation can be judged by the perception between pursuit-evasion USVs. For the pursuers, the model training is performed based on multi-agent deep reinforcement learning, so that they can quickly plan a reasonable obstacle avoidance and pursuit route, and form an effective encirclement posture before the evader approaches the target point. Meanwhile, the credit assignment problem among the members of the pursuing group is considered. For the evader, deep reinforcement learning is combined with imitation learning to train the escape model, so that it can reach the preset point in as short a time as possible and avoid the obstacles on the way. Finally, an adversarial-evolutionary game training method under multiple random scenarios is designed and combined with curriculum learning to iteratively update the pursuit and escape models. Through the detailed comparative analysis of the model training process and simulation experiments, it is proved that the proposed two types of models have higher convergence efficiency and stability, and they can have higher intelligence to pursue, escape and avoid obstacles respectively.}
}
@article{WANG202068,
title = {MRCDRL: Multi-robot coordination with deep reinforcement learning},
journal = {Neurocomputing},
volume = {406},
pages = {68-76},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.04.028},
url = {https://www.sciencedirect.com/science/article/pii/S0925231220305932},
author = {Di Wang and Hongbin Deng and Zhenhua Pan},
keywords = {Cooperative control, Machine learning, Image processing},
abstract = {This paper proposes a multi-robot cooperative algorithm based on deep reinforcement learning (MRCDRL). We use end-to-end methods to train directly from each robot-centered, relative perspective-generated image, and each robot’s reward as the input. During training, it is not necessary to specify the target position and movement path of each robot. MRCDRL learns the actions of each robot by training the neural network. MRCDRL uses the neural network structure that was modified from the Duel neural network structure. In the Duel network structure, there are two streams that each represents the state value function and the state-dependent action advantage function, and the results of the two streams are merged. The proposed method can solve the resource competition problem on the one hand and can solve the static and dynamic obstacle avoidance problems between multi-robot in real time on the other hand. Our new MRCDRL algorithm has higher accuracy and robustness than DQN and DDQN and can be effectively applied to multi-robot collaboration.}
}
@article{SUN2023106197,
title = {Event-triggered reconfigurable reinforcement learning motion-planning approach for mobile robot in unknown dynamic environments},
journal = {Engineering Applications of Artificial Intelligence},
volume = {123},
pages = {106197},
year = {2023},
issn = {0952-1976},
doi = {https://doi.org/10.1016/j.engappai.2023.106197},
url = {https://www.sciencedirect.com/science/article/pii/S0952197623003810},
author = {Huihui Sun and Changchun Zhang and Chunhe Hu and Junguo Zhang},
keywords = {Mobile robot, Reinforcement learning, Actor–critic, Reconfigurable structure, Sample pretreatment, Event-triggered},
abstract = {Deep reinforcement learning (DRL) is an essential technique for autonomous motion planning of mobile robots in dynamic and uncertain environments. In attempting to acquire a satisfactory DRL-based motion planning strategy, the mobile robots encountered several difficulties, including poor convergence, insufficient sample information, and low learning efficiency. These problems not only consume plenty of training time, but also bring a negative impact on motion planning performance. One promising research direction is to provide a more effective network framework for DRL-based policies. Along this line of thinking, our paper presents a novel DRL-based motion planning approach called Reconfigurable Structure of Deep Deterministic Policy Gradient (RS-DDPG) for mobile robots. To account for the poor convergence, the proposed approach first introduces an event-triggered reconfigurable actor–critic network framework for motion policy that adaptively changes its network structure to suppress the overestimation of action value. Then, the time convergence of the motion policy can be enhanced based on the value actions with minor valuation deviation. Afterwards, an adaptive reward mechanism is designed for reconfigurable networks to compensate for the lack of sample information. To deal with the problem of low learning efficiency, we developed a sample pretreatment method for the experience samples, which employs three novel techniques to improve the sample utilization, including a double experience memory buffer, a variable proportional sampling principle, and a similarity judgment mechanism. In extensive experiments, the proposed method outperforms the compared approaches.}
}
@article{YU202344,
title = {Hybrid attention-oriented experience replay for deep reinforcement learning and its application to a multi-robot cooperative hunting problem},
journal = {Neurocomputing},
volume = {523},
pages = {44-57},
year = {2023},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.12.020},
url = {https://www.sciencedirect.com/science/article/pii/S092523122201517X},
author = {Lingli Yu and Shuxin Huo and Zhengjiu Wang and Keyi Li},
keywords = {Deep reinforcement learning, Multi-robot, MADDPG, Prioritized experience replay, Attention mechanism},
abstract = {Multiple robots complete a cooperative hunting task by obtaining environmental information and autonomously learning hunting decision-making strategies. However, with the increase in the number of environment participants, it becomes difficult for robots to process a large amount of environmental information. Thus, a multi-robot cooperative hunting decision-making method called hybrid attention-oriented experience replay in multi-agent deep deterministic policy gradient (HAER-MADDPG) is proposed. First, a hybrid attention module is designed to pay greater attention to key information in a large amount of environmental information by integrating it with the multi-agent deep deterministic policy gradient (MADDPG). The method then combines hybrid attention and prioritized experience replay to improve the utilization of experience samples. Finally, the proposed algorithm is tested through a predator–prey game. The results show that the effectiveness, convergence speed, and scalability of the proposed algorithm are better than those of the baseline algorithms. In addition, HAER-MADDPG is effectively applied to a hunting task with real robots.}
}
@article{HILLEBRAND2020266,
title = {A design methodology for deep reinforcement learning in autonomous systems},
journal = {Procedia Manufacturing},
volume = {52},
pages = {266-271},
year = {2020},
note = {System-Integrated Intelligence – Intelligent, Flexible and Connected Systems in Products and ProductionProceedings of the 5th International Conference on System-Integrated Intelligence (SysInt 2020), Bremen, Germany},
issn = {2351-9789},
doi = {https://doi.org/10.1016/j.promfg.2020.11.044},
url = {https://www.sciencedirect.com/science/article/pii/S2351978920321879},
author = {Michael Hillebrand and Mohsin Lakhani and Roman Dumitrescu},
keywords = {Model-Based Systems Engineering, Autonomous Systems, Deep Reinforcement Learning, Hybrid Testbed},
abstract = {Autonomous systems such as mobile robots will play an important role in fields like industrial production, transportation or in hostile environments such as space. One of the most fundamental problem in autonomous mobile robotics is autonomous navigation. It is imperative for a mobile robot to learn to navigate in complex environments such as roads or buildings. The most popular approach to this problem is to utilize different algorithms for mapping the environment, self-localization in the map, planning a trajectory to the given goal and executing this trajectory. However, there are some drawbacks of these approaches. We often make assumptions about the environment such as no dynamic or transparent objects. Moreover, there is considerable overhead, they do not learn from failures and operation scenarios. This prompts us to search for alternative approaches for autonomous navigation, such as deep reinforcement learning. However, the application of deep reinforcement learning to a particular task involves a series of non-trivial design decisions. Previous work have failed to address the need for a design methodology for deep reinforcement learning systems. In this paper, we propose design methodology and discuss relevant design decisions for deep reinforcement learning in autonomous systems. We apply the methodology to the problem of autonomous navigation.}
}
@article{WEN2021107605,
title = {A multi-robot path-planning algorithm for autonomous navigation using meta-reinforcement learning based on transfer learning},
journal = {Applied Soft Computing},
volume = {110},
pages = {107605},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2021.107605},
url = {https://www.sciencedirect.com/science/article/pii/S1568494621005263},
author = {Shuhuan Wen and Zeteng Wen and Di Zhang and Hong Zhang and Tao Wang},
keywords = {Multi-robot system, Path planning, Deep reinforcement learning, Meta learning, Transfer learning},
abstract = {The adaptability of multi-robot systems in complex environments is a hot topic. Aiming at static and dynamic obstacles in complex environments, this paper presents dynamic proximal meta policy optimization with covariance matrix adaptation evolutionary strategies (dynamic-PMPO-CMA) to avoid obstacles and realize autonomous navigation. Firstly, we propose dynamic proximal policy optimization with covariance matrix adaptation evolutionary strategies (dynamic-PPO-CMA) based on original proximal policy optimization (PPO) to obtain a valid policy of obstacles avoidance. The simulation results show that the proposed dynamic-PPO-CMA can avoid obstacles and reach the designated target position successfully. Secondly, in order to improve the adaptability of multi-robot systems in different environments, we integrate meta-learning with dynamic-PPO-CMA to form the dynamic-PMPO-CMA algorithm. In training process, we use the proposed dynamic-PMPO-CMA to train robots to learn multi-task policy. Finally, in testing process, transfer learning is introduced to the proposed dynamic-PMPO-CMA algorithm. The trained parameters of meta policy are transferred to new environments and regarded as the initial parameters. The simulation results show that the proposed algorithm can have faster convergence rate and arrive the destination more quickly than PPO, PMPO and dynamic-PPO-CMA.}
}
@article{WU2020105201,
title = {The autonomous navigation and obstacle avoidance for USVs with ANOA deep reinforcement learning method},
journal = {Knowledge-Based Systems},
volume = {196},
pages = {105201},
year = {2020},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2019.105201},
url = {https://www.sciencedirect.com/science/article/pii/S0950705119305350},
author = {Xing Wu and Haolei Chen and Changgu Chen and Mingyu Zhong and Shaorong Xie and Yike Guo and Hamido Fujita},
keywords = {Autonomous navigation, Obstacle avoidance, Reinforcement learning, Unmanned surface vehicle (USV)},
abstract = {The unmanned surface vehicle (USV) has been widely used to accomplish missions in the sea or dangerous marine areas for ships with sailors, which greatly expands protective capability and detection range. When USVs perform various missions in sophisticated marine environment, autonomous navigation and obstacle avoidance will be necessary and essential. However, there are few effective navigation methods with real-time path planning and obstacle avoidance in dynamic environment. With tailored design of state and action spaces and a dueling deep Q-network, a deep reinforcement learning method ANOA (Autonomous Navigation and Obstacle Avoidance) is proposed for the autonomous navigation and obstacle avoidance of USVs. Experimental results demonstrate that ANOA outperforms deep Q-network (DQN) and Deep Sarsa in the efficiency of exploration and the speed of convergence not only in static environment but also in dynamic environment. Furthermore, the ANOA is integrated with the real control model of a USV moving in surge, sway and yaw and it achieves a higher success rate than Recast navigation method in dynamic environment.}
}
@article{KIM2023104715,
title = {Simulating travel paths of construction site workers via deep reinforcement learning considering their spatial cognition and wayfinding behavior},
journal = {Automation in Construction},
volume = {147},
pages = {104715},
year = {2023},
issn = {0926-5805},
doi = {https://doi.org/10.1016/j.autcon.2022.104715},
url = {https://www.sciencedirect.com/science/article/pii/S0926580522005854},
author = {Minguk Kim and Youngjib Ham and Choongwan Koo and Tae Wan Kim},
keywords = {Construction site, Construction worker, Site layout planning, Deep reinforcement learning, Pathfinding},
abstract = {Many optimization methods for construction site layout planning (CSLP) generate the shortest path of workers to calculate traveling costs and site safety performance. However, this approach often degrades the solution's reliability because workers in real-life situations do not necessarily take the shortest path to their chosen destination. Thus, this paper proposes a novel approach for generating realistic paths that mimic their wayfinding decision-making process. This approach uses deep reinforcement learning, for which the framework to facilitate its use includes the following elements: (1) the required properties and functions for site objects; and (2) the state, action space, and reward functions intended. The similarity between the paths simulated and the real workers' trajectories has been validated better by 17.8% than the traditional A* algorithm. The proposed approach is expected to be used as an appropriate input, and thereby help improve the reliability of the solutions based on the CSLP optimization methods.}
}
@article{YANG202219,
title = {Dynamic Path Planning for Mobile Robots with Deep Reinforcement Learning},
journal = {IFAC-PapersOnLine},
volume = {55},
number = {11},
pages = {19-24},
year = {2022},
note = {IFAC Workshop on Control for Smart Cities CSC 2022},
issn = {2405-8963},
doi = {https://doi.org/10.1016/j.ifacol.2022.08.042},
url = {https://www.sciencedirect.com/science/article/pii/S2405896322011302},
author = {Laiyi Yang and Jing Bi and Haitao Yuan},
keywords = {Deep reinforcement learning, path planning, Soft Actor-Critic algorithm, continuous reward functions, mobile robots},
abstract = {Traditional path planning algorithms for mobile robots are not effective to solve high-dimensional problems, and suffer from slow convergence and complex modelling. Therefore, it is highly essential to design a more efficient algorithm to realize intelligent path planning of mobile robots. This work proposes an improved path planning algorithm, which is based on the algorithm of Soft Actor-Critic (SAC). It attempts to solve a problem of poor robot performance in complicated environments with static and dynamic obstacles. This work designs an improved reward function to enable mobile robots to quickly avoid obstacles and reach targets by using state dynamic normalization and priority replay buffer techniques. To evaluate its performance, a Pygame-based simulation environment is constructed. The proposed method is compared with a Proximal Policy Optimization (PPO) algorithm in the simulation environment. Experimental results demonstrate that the cumulative reward of the proposed method is much higher than that of PPO, and it is also more robust than PPO.}
}
@article{XIAO2023102440,
title = {Multimodal fusion for autonomous navigation via deep reinforcement learning with sparse rewards and hindsight experience replay},
journal = {Displays},
volume = {78},
pages = {102440},
year = {2023},
issn = {0141-9382},
doi = {https://doi.org/10.1016/j.displa.2023.102440},
url = {https://www.sciencedirect.com/science/article/pii/S0141938223000732},
author = {Wendong Xiao and Liang Yuan and Teng Ran and Li He and Jianbo Zhang and Jianping Cui},
keywords = {Hindsight experience replay, Obstacle avoidance, Deep reinforcement learning, Sparse rewards, Multimodal navigation},
abstract = {The multimodal perception of intelligent robots is essential for achieving collision-free and efficient navigation. Autonomous navigation is enormously challenging when perception is acquired using only vision or LiDAR sensor data due to the lack of complementary information from different sensors. This paper proposes a simple yet efficient deep reinforcement learning (DRL) with sparse rewards and hindsight experience replay (HER) to achieve multimodal navigation. By adopting the depth images and pseudo-LiDAR data generated by an RGB-D camera as input, a multimodal fusion scheme is used to enhance the perception of the surrounding environment compared to using a single sensor. To alleviate the misleading way for the agent to navigate with dense rewards, the sparse rewards are intended to identify its tasks. Additionally, the HER technique is introduced to address the sparse reward navigation issue for accelerating optimal policy learning. The results show that the proposed model achieves state-of-the-art performance in terms of success, crash, and timeout rates, as well as generalization capability.}
}
@article{LI202375,
title = {Deep reinforcement learning in smart manufacturing: A review and prospects},
journal = {CIRP Journal of Manufacturing Science and Technology},
volume = {40},
pages = {75-101},
year = {2023},
issn = {1755-5817},
doi = {https://doi.org/10.1016/j.cirpj.2022.11.003},
url = {https://www.sciencedirect.com/science/article/pii/S1755581722001717},
author = {Chengxi Li and Pai Zheng and Yue Yin and Baicun Wang and Lihui Wang},
keywords = {Deep reinforcement learning, Smart manufacturing, Engineering life cycle, Artificial intelligence, Review},
abstract = {To facilitate the personalized smart manufacturing paradigm with cognitive automation capabilities, Deep Reinforcement Learning (DRL) has attracted ever-increasing attention by offering an adaptive and flexible solution. DRL takes the advantages of both Deep Neural Networks (DNN) and Reinforcement Learning (RL), by embracing the power of representation learning, to make precise and fast decisions when facing dynamic and complex situations. Ever since the first paper of DRL was published in 2013, its applications have sprung up across the manufacturing field with exponential publication growth year by year. However, there still lacks any comprehensive review of the DRL in the field of smart manufacturing. To fill this gap, a systematic review process was conducted, with 261 relevant publications selected to date (20-Oct-2022), to gain a holistic understanding of the development, application, and challenges of DRL in smart manufacturing along the whole engineering lifecycle. First, the concept and development of DRL are summarized. Then, the typical DRL applications are analyzed in the four engineering lifecycle stages: design, manufacturing, distribution, and maintenance. Finally, the challenges and future directions are illustrated, especially emerging DRL-related technologies and solutions that can improve the manufacturing system’s deployment feasibility, cognitive capability, and learning efficiency, respectively. It is expected that this work can provide an insightful guide to the research of DRL in the smart manufacturing field and shed light on its future perspectives.}
}
@article{CARLUCHO2020280,
title = {An adaptive deep reinforcement learning approach for MIMO PID control of mobile robots},
journal = {ISA Transactions},
volume = {102},
pages = {280-294},
year = {2020},
issn = {0019-0578},
doi = {https://doi.org/10.1016/j.isatra.2020.02.017},
url = {https://www.sciencedirect.com/science/article/pii/S0019057820300781},
author = {Ignacio Carlucho and Mariano {De Paula} and Gerardo G. Acosta},
keywords = {Reinforcement learning, Adaptive control, Policy gradient, Mobile robots, Multi-platforms},
abstract = {Intelligent control systems are being developed for the control of plants with complex dynamics. However, the simplicity of the PID (proportional–integrative–derivative) controller makes it still widely used in industrial applications and robotics. This paper proposes an intelligent control system based on a deep reinforcement learning approach for self-adaptive multiple PID controllers for mobile robots. The proposed hybrid control strategy uses an actor–critic structure and it only receives low-level dynamic information as input and simultaneously estimates the multiple parameters or gains of the PID controllers. The proposed approach was tested in several simulated environments and in a real time robotic platform showing the feasibility of the approach for the low-level control of mobile robots. From the simulation and experimental results, our proposed approach demonstrated that it can be of aid by providing with behavior that can compensate or even adapt to changes in the uncertain environments providing a model free unsupervised solution. Also, a comparative study against other adaptive methods for multiple PIDs tuning is presented, showing a successful performance of the approach.}
}
@article{DIN2022108089,
title = {A deep reinforcement learning-based multi-agent area coverage control for smart agriculture},
journal = {Computers and Electrical Engineering},
volume = {101},
pages = {108089},
year = {2022},
issn = {0045-7906},
doi = {https://doi.org/10.1016/j.compeleceng.2022.108089},
url = {https://www.sciencedirect.com/science/article/pii/S0045790622003445},
author = {Ahmad Din and Muhammed Yousoof Ismail and Babar Shah and Mohammad Babar and Farman Ali and Siddique Ullah Baig},
keywords = {Area coverage, Smart sensors, Deep reinforcement learning, Smart agriculture, Precision agriculture, Multi-robotics systems, Internet of agricultural things (IoAT)},
abstract = {Precision agriculture (PA) is a collage of strategies and technologies to optimize operations and decisions in farms by using spatial and temporal variabilities in yield, crops, and soil within an agricultural plot. It is a data-driven technique, therefore, selective treatment of crops and soil, and managing variabilities using robots and smart sensors is the next improvement in PA. In this paper, it is modeled as a multi-agent patrolling problem, where robots visit subregions that required immediate attention in the agricultural field. Furthermore, for area coverage / patrolling task in the agricultural plot, a centralized Convolutional Neural Network (CNN) based Dual Deep Q-learning (DDQN) is proposed. A customized reward function is designed, which rewards worth-visiting idle regions, and punishes undesirable actions. A proposed algorithm has been compared with various algorithms including individual Q-learning (IRL), uniform coverage (UC), and Behavior-Based Robotics coverage (BBR) for different scenarios in the agricultural plots.}
}
@article{JIANG2022118,
title = {iTD3-CLN: Learn to navigate in dynamic scene through Deep Reinforcement Learning},
journal = {Neurocomputing},
volume = {503},
pages = {118-128},
year = {2022},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2022.06.102},
url = {https://www.sciencedirect.com/science/article/pii/S0925231222008347},
author = {Haoge Jiang and Mahdi Abolfazli Esfahani and Keyu Wu and Kong-wah Wan and Kuan-kian Heng and Han Wang and Xudong Jiang},
keywords = {Deep reinforcement learning, Collision avoidance, Motion and path planning, Real-time autonomous navigation, Autonomous unmanned vehicles},
abstract = {This paper proposes iTD3-CLN, a Deep Reinforcement Learning (DRL) based low-level motion controller, to achieve map-less autonomous navigation in dynamic scene. We consider three enhancements to the Twin Delayed DDPG (TD3) for the navigation task: N-step returns, Priority Experience Replay, and a channel-based Convolutional Laser Network (CLN) architecture. In contrast to the conventional methods such as the DWA, our approach is found superior in the following ways: no need for prior knowledge of the environment and metric map, lower reliance on an accurate sensor, learning emergent behavior in dynamic scene that is intuitive, and more remarkably, able to transfer to the real robot without further fine-tuning. Our extensive studies show that in comparison to the original TD3, the proposed approach can obtain approximately 50% reduction in training to get same performance, 50% higher accumulated reward, and 30–50% increase in generalization performance when tested in unseen environments. Videos of our experiments are available at https://youtu.be/BRN0Gk5oLOc (Simulation) and https://youtu.be/yIxGH9TPQCc (Real experiment).}
}
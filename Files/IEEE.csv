"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Deep reinforcement learning based mobile robot navigation: A review","K. Zhu; T. Zhang","Department of Automation, Tsinghua University, Beijing, China; Department of Automation, Tsinghua University, Beijing, China","Tsinghua Science and Technology","20 Apr 2021","2021","26","5","674","691","Navigation is a fundamental problem of mobile robots, for which Deep Reinforcement Learning (DRL) has received significant attention because of its strong representation and experience learning abilities. There is a growing trend of applying DRL to mobile robot navigation. In this paper, we review DRL methods and DRL-based navigation frameworks. Then we systematically compare and analyze the relationship and differences between four typical application scenarios: local obstacle avoidance, indoor navigation, multi-robot navigation, and social navigation. Next, we describe the development of DRL-based navigation. Last, we discuss the challenges and some possible solutions regarding DRL-based navigation.","1007-0214","","10.26599/TST.2021.9010012","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9409758","mobile robot navigation;obstacle avoidance;deep reinforcement learning","Navigation;Mobile robots;Reinforcement learning;Simultaneous localization and mapping;Robots;Task analysis;Robot sensing systems","collision avoidance;control engineering computing;deep learning (artificial intelligence);mobile robots;multi-robot systems;navigation","fundamental problem;mobile robots;strong representation;experience learning abilities;mobile robot navigation;DRL methods;DRL-based navigation frameworks;indoor navigation;multirobot navigation;social navigation;deep reinforcement learning","","64","","","","20 Apr 2021","","","TUP","TUP Journals"
"Experimental Research on Deep Reinforcement Learning in Autonomous navigation of Mobile Robot","P. Yue; J. Xin; H. Zhao; D. Liu; M. Shan; J. Zhang","Shaanxi Key Laboratory of Integrated and Intelligent Navigation, CETC 20, Xi'an, China; Shaanxi Key Laboratory of Integrated and Intelligent Navigation, CETC 20, Xi'an, China; Xi'an University of Technology; Xi'an University of Technology, Xi'an, Shaanxi, CN; Xi'an University of Technology; Australian Centre for Field Robotics, The University of Sydney, Sydney, Australia","2019 14th IEEE Conference on Industrial Electronics and Applications (ICIEA)","16 Sep 2019","2019","","","1612","1616","The paper is concerned with the autonomous navigation of mobile robot from the current position to the desired position only using the current visual observation, without the environment map built beforehand. Under the framework of deep reinforcement learning, the Deep Q Network (DQN) is used to achieve the mapping from the original image to the optimal action of the mobile robot. Reinforcement learning requires a large number of training examples, which is difficult to directly be applied in a real robot navigation scenario. To solve the problem, the DQN is firstly trained in the Gazebo simulation environment, followed by the application of the well-trained DQN in the real mobile robot navigation scenario. Both simulation and real-world experiments have been conducted to validate the proposed approach. The experimental results of mobile robot autonomous navigation in the Gazebo simulation environment show that the trained DQN can approximate the state action value function of the mobile robot and perform accurate mapping from the current original image to the optimal action of the mobile robot. The experimental results in real indoor scenes demonstrate that the DQN trained in the simulated environment can work in the real indoor environment, and the mobile robot can also avoid obstacles and reach the target location even with dynamics and the presence of interference in the environment. It is therefore an effective and environmentally adaptable autonomous navigation method for mobile robots in an unknown environment.","2158-2297","978-1-5386-9490-9","10.1109/ICIEA.2019.8833968","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8833968","Deep reinforcement learning;DQN;mobile robot;nature scene;autonomous navigation","Mobile robots;Autonomous robots;Reinforcement learning;Training;Navigation;Games","collision avoidance;control engineering computing;learning (artificial intelligence);mobile robots;navigation;robot vision","deep reinforcement learning;DQN;mobile robot autonomous navigation;visual observation;deep Q network;Gazebo simulation environmen;indoor environment;obstacle avoidance","","6","","16","IEEE","16 Sep 2019","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning for Mobile Robot Navigation","M. Gromniak; J. Stenzel","Institute of Medical Technology, Technical University of Hamburg, Hamburg, Germany; Department of Automation and Embedded Systems, Fraunhofer Institute for Material Flow and Logistics, Dortmund, Germany","2019 4th Asia-Pacific Conference on Intelligent Robot Systems (ACIRS)","19 Dec 2019","2019","","","68","73","While navigation is arguable the most important aspect of mobile robotics, complex scenarios with dynamic environments or with teams of cooperative robots are still not satisfactory solved yet. Motivated by the recent successes in the reinforcement learning domain, the application of deep reinforcement learning to robot navigation was examined in this paper. In particular this required the development of a training procedure, a set of actions available to the robot, a suitable state representation and a reward function. The setup was evaluated using a simulated real-time environment. A reference setup, different goal-oriented exploration strategies and two different robot kinematics (holonomic, differential) were compared in the evaluation. In a challenging scenario with obstacles at changing locations in the environment the robot was able to reach the desired goal in 93% of the episodes.","","978-1-7281-2229-8","10.1109/ACIRS.2019.8935944","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8935944","reinforcement learning;end-to-end-learning;mobile robot navigation;robot learning","Navigation;Reinforcement learning;Mobile robots;Robot sensing systems;Neural networks;Task analysis","control engineering computing;learning (artificial intelligence);mobile robots;navigation;neurocontrollers;path planning;robot kinematics;robot programming","deep reinforcement learning;mobile robot navigation;mobile robotics;dynamic environments;reinforcement learning domain;state representation;real-time environment;robot kinematics;goal-oriented exploration strategies;reward function;holonomic robot kinematic;differential robot kinematic","","4","","12","IEEE","19 Dec 2019","","","IEEE","IEEE Conferences"
"Socially Aware Robot Navigation Using Deep Reinforcement Learning","T. Xuan Tung; T. Dung Ngo","Truong Xuan Tung and Trung Dung Ngo are with The More-Than-One Robotics Laboratory, University of Prince Edward Island, Canada; Truong Xuan Tung and Trung Dung Ngo are with The More-Than-One Robotics Laboratory, University of Prince Edward Island, Canada","2018 IEEE Canadian Conference on Electrical & Computer Engineering (CCECE)","30 Aug 2018","2018","","","1","5","In this study, we propose a socially aware navigation framework for mobile service robots in dynamic human environments using a deep reinforcement learning algorithm. The primary idea of the proposed algorithm is to incorporate obstacles information (position and motion), human states (human position, human motion), social interactions (human group, human-object interaction), and social rules, e.g, minimum distances from the robot to regular obstacles, individuals, and human groups into the deep reinforcement learning model of a mobile robot. We then distribute the mobile robot into a dynamic social environment and let the mobile robot automatically learn to adapt to an embedded environment by its experiences gained through trial-and-error social interactions with the surrounding humans and objects. When the learning phase is completed, the mobile robot is able to navigate autonomously in the social environment while guaranteeing human safety and comfort with its socially acceptable behaviours.","2576-7046","978-1-5386-2410-4","10.1109/CCECE.2018.8447854","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8447854","Socially aware robot navigation;social robots;mobile service robots;deep reinforcement learning. Socially aware robot navigation;deep reinforcement learning.S","Navigation;Mobile robots;Learning (artificial intelligence);Machine learning;Heuristic algorithms;Service robots","collision avoidance;human-robot interaction;learning (artificial intelligence);mobile robots;service robots","aware robot navigation;socially aware navigation framework;mobile service robots;dynamic human environments;deep reinforcement learning algorithm;human states;human position;human motion;human group;social rules;deep reinforcement learning model;mobile robot;dynamic social environment;learning phase;human safety;socially acceptable behaviours;trial-and-error social interactions","","4","","25","IEEE","30 Aug 2018","","","IEEE","IEEE Conferences"
"Mobile Robot Navigation based on Deep Reinforcement Learning","X. Ruan; D. Ren; X. Zhu; J. Huang","Beijing Key Laboratory of Computational Intelligence and Intelligent System, Beijing, China; Beijing Key Laboratory of Computational Intelligence and Intelligent System, Beijing, China; Beijing Key Laboratory of Computational Intelligence and Intelligent System, Beijing, China; Beijing Key Laboratory of Computational Intelligence and Intelligent System, Beijing, China","2019 Chinese Control And Decision Conference (CCDC)","12 Sep 2019","2019","","","6174","6178","Learning to navigate in an unknown environment is a crucial capability of mobile robot. Conventional method for robot navigation consists of three steps, involving localization, map building and path planning. However, most of the conventional navigation methods rely on obstacle map, and dont have the ability of autonomous learning. In contrast to the traditional approach, we propose an end-to-end approach in this paper using deep reinforcement learning for the navigation of mobile robots in an unknown environment. Based on dueling network architectures for deep reinforcement learning (Dueling DQN) and deep reinforcement learning with double q learning (Double DQN), a dueling architecture based double deep q network (D3QN) is adapted in this paper. Through D3QN algorithm, mobile robot can learn the environment knowledge gradually through its wonder and learn to navigate to the target destination autonomous with an RGB-D camera only. The experiment results show that mobile robot can reach to the desired targets without colliding with any obstacles.","1948-9447","978-1-7281-0106-4","10.1109/CCDC.2019.8832393","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8832393","Robot Navigation;Deep Reinforcement Learning;Depth Image","Navigation;Robot sensing systems;Reinforcement learning;Deep learning;Mobile robots;Collision avoidance","collision avoidance;control engineering computing;image colour analysis;learning (artificial intelligence);mobile robots;navigation;neural nets;robot programming;robot vision","deep reinforcement learning;double q learning;mobile robot navigation;autonomous learning;path planning;map building;dueling architecture based double deep q network;D3QN algorithm","","38","","16","IEEE","12 Sep 2019","","","IEEE","IEEE Conferences"
"Continuous Control with Deep Reinforcement Learning for Mobile Robot Navigation","J. Xiang; Q. Li; X. Dong; Z. Ren","School of Automation Science and Electrical Engineering, Beihang University, Beijing, P.R. China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, P.R. China; School of Automation Science and Electrical Engineering, Beihang University, Beijing, P.R. China; Beijing Advanced Innovation Center for Big Data and Brain Computing, Beihang University, Beijing, P.R. China","2019 Chinese Automation Congress (CAC)","13 Feb 2020","2019","","","1501","1506","Autonomous navigation is one of the focuses in the field of mobile robot research. The traditional method usually consists of two parts: building the map of environment, localization of mobile robot and path planning. However, these traditional methods usually rely on high-precision sensor information. At the same time, mobile robots have no intelligent understanding of autonomous navigation. In this article, a deep reinforcement learning method, i.e. soft actor critic, is used to navigate in a mapless environment. It takes laser scanning data and information of the target as input, outputs linear velocity and angular velocity in continuous space. The simulation shows that this learning-based end-to-end autonomous navigation method can accomplish tasks as well as traditional methods.","2688-0938","978-1-7281-4094-0","10.1109/CAC48633.2019.8996652","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8996652","Mobile Robot;Deep Reinforcement Learning;Soft Actor Critic;Autonomous navigation","Mobile robots;Machine learning;Navigation;Autonomous robots;Entropy;Lasers","angular velocity control;control engineering computing;learning (artificial intelligence);mobile robots;navigation;path planning;robot vision","continuous control;mobile robot navigation;mobile robot research;path planning;high-precision sensor information;mobile robots;intelligent understanding;deep reinforcement learning method;mapless environment;laser scanning data;continuous space;learning-based end-to-end autonomous navigation method","","10","","17","IEEE","13 Feb 2020","","","IEEE","IEEE Conferences"
"Deep Reinforcement Learning Based Mobile Robot Navigation in Unknown Environment with Continuous Action Space","P. Phueakthong; J. Varagul; N. Pinrath","School of Mechatronics Engineering, Suranaree University of Technology, Nakhon Ratchasima, Thailand; School of Manufacturing Automation and Robotics Engineering Suranaree University of Technology, Nakhon Ratchasima, Thailand; School of Industrial Engineering Suranaree University of Technology, Nakhon Ratchasima, Thailand","2022 5th International Conference on Intelligent Autonomous Systems (ICoIAS)","8 Nov 2022","2022","","","154","158","This work aims to propose the use of deep reinforcement learning for mobile robot navigation and obstacle avoidance in previously unknown areas or without pre-made maps with continuous action control to increase the capabilities of mobile robots beyond conventional map-based navigation. Deep reinforcement learning is used to enable the robot to learn how to make decisions and interact with the environment observed from sensor data to safely navigate itself to its destination. The robot has a two-dimensional laser scanner, ultrasonic sensors and odometry sensor. Deep Deterministic Policy Gradient, which can function in continuous action space, was chosen as the deep reinforcement learning model. The robot is trained and tested in a Gazebo simulator with Robot Operating System. After the training process, the robot is put to the challenge to complete a waypoint navigation mission in four unknown areas as part of an assessment. The results indicate that the mobile robot is adaptable and has the capability of traveling to the specified waypoints and completing the job without the need for a pre-drawn route or an obstacle map in unidentified environments with the minimum success rate of 69.7 percent.","","978-1-6654-9838-8","10.1109/ICoIAS56028.2022.9931272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9931272","deep reinforcement learning;autonomous navigation;DDPG","Training;Navigation;Autonomous systems;Operating systems;Reinforcement learning;Robot sensing systems;Laser modes","collision avoidance;control engineering computing;deep learning (artificial intelligence);mobile robots;navigation;reinforcement learning;robot programming","continuous action control;continuous action space;deep deterministic policy gradient;deep reinforcement learning model;Gazebo simulator;mobile robot navigation;obstacle avoidance;odometry sensor;robot operating system;training process;two-dimensional laser scanner;ultrasonic sensors;unknown areas;unknown environment;waypoint navigation mission","","","","16","IEEE","8 Nov 2022","","","IEEE","IEEE Conferences"
"End-to-End Mobile Robot Navigation using a Residual Deep Reinforcement Learning in Dynamic Human Environments","A. Ahmed; Y. F. O. Mohammad; V. Parque; H. El-Hussieny; S. Ahmed","Department of Mechatronics and Robotics Engineering, Egypt-Japan University of Science and Technology, Alexandria, Egypt; National Institute of Advanced Industrial Science and Technology, Tokyo, Japan; Department of Modern Mechanical Engineering, Waseda University, Tokyo, Japan; Department of Mechatronics and Robotics Engineering, Egypt-Japan University of Science and Technology, Alexandria, Egypt; Department of Mechatronics and Robotics Engineering, Egypt-Japan University of Science and Technology, Alexandria, Egypt","2022 18th IEEE/ASME International Conference on Mechatronic and Embedded Systems and Applications (MESA)","10 Jan 2023","2022","","","1","6","Safe navigation through human crowds is key to enabling practical mobility ubiquitously. The Deep Reinforcement Learning (DRL) and the End-to-End (E2E) approaches to goal-oriented robot navigation have the potential to render policies able to tackle localization, path planning, obstacle avoidance, and adaptation to change in unison. In this paper, we report an architecture based on convolutional units and residual blocks being able to enhance adaptability to unseen and dynamic human environments. In particular, our scheme outperformed the state-of-the-art baselines SOADRL and NAVREP by about 13% and 18% on average success rate, respectively, throughout 27 unseen and dynamic navigation instances. Furthermore, our approach avoids the explicit encoding of positions and trajectories of moving humans compared to the standard models. Our results show the potential to render adaptive and generalizable policies for unknown and dynamic human environments.","","978-1-6654-5570-1","10.1109/MESA55290.2022.10004394","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10004394","Autonomous Navigation;Mobile Robots;End-to-End Learning;Deep Reinforcement Learning;Convolutional Neural Networks;Dynamic Environments","Deep learning;Training;Adaptation models;Navigation;Computational modeling;Reinforcement learning;Data models","collision avoidance;deep learning (artificial intelligence);mobile robots;reinforcement learning","adaptive policies;dynamic human environments;dynamic navigation instances;end-to-end mobile robot navigation;generalizable policies;goal-oriented robot navigation;human crowds;NAVREP;residual blocks;residual deep reinforcement learning;safe navigation;SOADRL","","","","18","IEEE","10 Jan 2023","","","IEEE","IEEE Conferences"
"Asynchronous deep reinforcement learning for the mobile robot navigation with supervised auxiliary tasks","T. Tongloy; S. Chuwongin; K. Jaksukam; C. Chousangsuntorn; S. Boonsang","Center of Industrial Robots and Automation (CiRA), King Mongkut's Institute of Technology Ladkrabang, Bangkok, Thailand; Center of Industrial Robots and Automation (CiRA), King Mongkut's Institute of Technology Ladkrabang, Bangkok, Thailand; King Mongkut's Institute of Technology Ladkrabang, Bangkok, TH; Department of Electrical Engineering, King Mongkut's Institute of Technology Ladkrabang, Bangkok, Thailand; Department of Electrical Engineering, King Mongkut's Institute of Technology Ladkrabang, Bangkok, Thailand","2017 2nd International Conference on Robotics and Automation Engineering (ICRAE)","15 Feb 2018","2017","","","68","72","In this paper, we present the method based on asynchronous deep reinforcement learning adapted for the mobile robot navigation with supervised auxiliary tasks. We apply the hybrid Asynchronous Advantage Actor-Critic (A3C) algorithm CPU/GPU based on TensorFlow. The mobile robot is simulated as the navigation tasks on the OpenAI-Gym-Gazebo-based environment with the collaboration with ROS Multimaster. The supervised auxiliary tasks include the depth predictions and the robot position estimation. The simulated mobile robot shows the capability to learn to navigate only the input from raw RGB-image and also perform recognition of the place on the map. We also show that the combination of all possible auxiliary tasks leads to the different learning rate.","","978-1-5386-1306-1","10.1109/ICRAE.2017.8291355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8291355","component;asynchronous reinforcement learning;GA3C;ROS;supervised auxiliary tasks;mobile robot navigation","Task analysis;Navigation;Mobile robots;Computational modeling;Adaptation models;Machine learning","control engineering computing;learning (artificial intelligence);mobile robots;path planning","learning rate;hybrid asynchronous advantage actor;OpenAI-Gym-Gazebo-based environment;ROS multimaster;TensorFlow;depth prediction;navigation tasks;supervised auxiliary tasks;mobile robot navigation;asynchronous deep reinforcement learning;simulated mobile robot;robot position estimation","","10","","12","IEEE","15 Feb 2018","","","IEEE","IEEE Conferences"
"Obstacle avoidance navigation method for robot based on deep reinforcement learning","X. Ruan; C. Lin; J. Huang; Y. Li","Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China; Faculty of Information Technology, Beijing University of Technology, Beijing, China","2022 IEEE 6th Information Technology and Mechatronics Engineering Conference (ITOEC)","23 Mar 2022","2022","6","","1633","1637","Aiming at the navigation problem of mobile robots indoor environment, the traditional navigation algorithm based on D3QN has some problems such as sparse reward and slow training speed of the neural network. This paper proposes a deep reinforcement learning Algorithm (LN-D3QN) based on the D3QN network to realize collision-free autonomous navigation of mobile robots. To improve the efficiency of mobile robot learning and exploration, the Vision Sensor is used to acquire the input data from the environment, and the layer normalization method is used to normalize the input data. An improved reward function is designed, which improves the reward value of the algorithm, optimizes the state space, and alleviates the problem of sparse reward to some extent. The data is stored in a priority experience replay pool, and the network is trained using small batches of data. In addition, we evaluate our method by experiment related to indoor navigation. The experiments show that the robot trained by the improved D3QN algorithm can adapt to the unknown environment more quickly than the basic D3QN algorithm. The network's convergence speed is also improved, and it can complete the obstacle avoidance navigation task more efficiently.","2693-289X","978-1-6654-3185-9","10.1109/ITOEC53115.2022.9734337","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9734337","obstacle avoidance;reinforcement learning;Autonomous navigation;path planning;deep learning","Training;Mechatronics;Navigation;Neural networks;Reinforcement learning;Vision sensors;Mobile robots","collision avoidance;deep learning (artificial intelligence);mobile robots;navigation;path planning;reinforcement learning;robot vision","convergence speed;navigation problem;mobile robots indoor environment;sparse reward;neural network;LN-D3QN;D3QN network;collision-free autonomous navigation;mobile robot learning;vision sensor;layer normalization;reward value;priority experience replay pool;indoor navigation;deep reinforcement learning;reward function;robot obstacle avoidance navigation","","","","22","IEEE","23 Mar 2022","","","IEEE","IEEE Conferences"
"Goal-Oriented Navigation with Avoiding Obstacle based on Deep Reinforcement Learning in Continuous Action Space","P. X. Hien; G. -W. Kim","Department of Control and Robot Engineering, Chungbuk National University, South Korea; Department of Intelligent System and Robotics, Chungbuk National University, South Korea","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","8","11","Obstacle avoidance problems using Deep Reinforcement Learning (DRL) are becoming possible solutions for autonomous mobile robots. In real-world situations with stationary and moving obstacles, mobile robots must be able to navigate to a goal and safely avoid collisions. This work is an extension of ongoing research on the navigation approach for a mobile robot. We show that through the proposed DRL, a goal-oriented collision avoidance model can be trained end-to-end without manual turning or supervision by a human operator. We suggest performing the obstacle avoidance algorithm of the mobile robot in both simulated environments and continuous action space of the real world. Finally, we measure and evaluate the obstacle avoidance capability through data collection of hit ratio metrics during robot execution.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9649898","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649898","deep reinforcement learning;Q-learning;obstacle avoidance;robot sensing systems;path planning","Measurement;Navigation;Reinforcement learning;Aerospace electronics;Robot sensing systems;Turning;Sensors","collision avoidance;deep learning (artificial intelligence);mobile robots;navigation;reinforcement learning","goal-oriented navigation;continuous action space;DRL;autonomous mobile robots;navigation approach;goal-oriented collision avoidance model;obstacle avoidance algorithm;obstacle avoidance capability;robot execution;deep reinforcement learning;hit ratio metrics","","1","","15","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Environment Exploration for Mapless Navigation based on Deep Reinforcement Learning","N. D. Toan; K. Gon-Woo","Department of Control and Robot Engineering, Chungbuk National University, Chungbuk, Korea; Department of Intelligent Systems and Robotics, Chungbuk National University, Chungbuk, Korea","2021 21st International Conference on Control, Automation and Systems (ICCAS)","28 Dec 2021","2021","","","17","20","In recent years, reinforcement learning has attracted researchers' attention with the AlphaGo event. Especially in autonomous mobile robots, the reinforcement learning approach can be applied to the mapless navigation problem. The Robot can complete the set tasks well and works well in different environments without maps and ready-made path plans. However, for reinforcement learning in general and mapless navigation based on reinforcement learning in particular, exploitation and exploration balance are issues that need to be carefully considered. Specifically, the fact that the agent (Robot) can discover and execute actions in a particular working environment plays a significant role in improving the performance of the reinforcement learning problem. By creating some noise during the convolutional neural network training, the above problem can be solved by some popular approaches today. With outstanding advantages compared to other approaches, the Boltzmann policy approach has been used in our problem. It helps the Robot explore more thoroughly in complex environments, and the policy is also more optimized.","2642-3901","978-89-93215-21-2","10.23919/ICCAS52745.2021.9649893","Ministry of Trade, Industry, and Energy(MOTIE); Korea Institute for Advancement of Technology(KIAT)(grant numbers:P0004631); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9649893","reinforcement learning;mapless navigation;exploitation;exploration balancing;Boltzmann policy","Training;Automation;Navigation;Reinforcement learning;Control systems;Mobile robots;Convolutional neural networks","Boltzmann machines;control engineering computing;convolutional neural nets;deep learning (artificial intelligence);intelligent robots;mobile robots;navigation;path planning;reinforcement learning;robot vision","mapless navigation problem;exploration balance;environment exploration;deep reinforcement learning;autonomous mobile robots;AlphaGo event;convolutional neural network training;Boltzmann policy approach","","","","9","","28 Dec 2021","","","IEEE","IEEE Conferences"
"Mapless Navigation with Deep Reinforcement Learning based on The Convolutional Proximal Policy Optimization Network","N. D. Toan; K. G. Woo","Dept. of Control and Robot Engineering Cheongju,Chungbuk,Korea; Dept. of Control and Robot Engineering Cheongju,Chungbuk,Korea","2021 IEEE International Conference on Big Data and Smart Computing (BigComp)","10 Mar 2021","2021","","","298","301","In recent years, mapless navigation with a deep reinforcement learning approach in Autonomous Mobile Robot has shown a considerable benefit in improving robot behavior flexibility. Specifically, the Robot adapts to complex constraints and performs well in various environments without the need for a predetermined map and route plan. However, several previous studies show a lack of stability in the training of deep reinforcement learning networks for mapless navigation. Moreover, the exploration and exploitation in a specific work environment play an essential role in improving mapless navigation performance, which needs to be carefully considered. From the aforementioned issues, as well as inspired by the Proximal Policy optimization algorithm has shown that it does not only evaluates the advantages and disadvantages of policy but also prevents the policy from changing too much after each time weight update. In this paper, we propose to build a Convolutional Proximal Policy optimization network for the Mapless Navigation problem. Furthermore, the use of Boltzmann's policy to help balance exploration and exploitation also contributes to the Robot's ability to explore more deeply in complex environments, and the performance of the mapless navigation problem is also significantly improved.","2375-9356","978-1-7281-8924-6","10.1109/BigComp51126.2021.00063","Technology Development; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9373298","reinforcement learning;mapless navigation;proximal policy optimization","Training;Navigation;Conferences;Reinforcement learning;Big Data;Mobile robots;Optimization","convolutional neural nets;deep learning (artificial intelligence);mobile robots;navigation;optimisation;path planning;robot vision","complex environments;mapless navigation problem;autonomous mobile robot;robot behavior flexibility;deep reinforcement learning networks;Boltzmann policy;robot ability;convolutional proximal policy optimization network;predetermined map route plan","","4","","13","IEEE","10 Mar 2021","","","IEEE","IEEE Conferences"
"Development of a New Intelligent Mobile Robot Path Planning Algorithm Based on Deep Reinforcement Learning Considering Pedestrian Traffic Rules","K. Kubota; K. Kobayashi; T. Ohkubo; K. Watanabe; N. J. Sebi; K. Tian; K. C. Cheok","Graduate School of Science and Engineering, Hosei University, Tokyo, Japan; Graduate School of Science and Engineering, Hosei University, Tokyo, Japan; Advanced Institute of Industrial Technology, Tokyo, Japan; Graduate School of Science and Engineering, Hosei University, Tokyo, Japan; Electrical & Computer Engineering Department, School of Engineering & Computer Science, Oakland University, Rochester, MI, USA; Electrical & Computer Engineering Department, School of Engineering & Computer Science, Oakland University, Rochester, MI, USA; Electrical & Computer Engineering Department, School of Engineering & Computer Science, Oakland University, Rochester, MI, USA","2022 61st Annual Conference of the Society of Instrument and Control Engineers (SICE)","6 Oct 2022","2022","","","628","632","In the near future in urban areas, people and mobile robots will coexist, and several strategies to avoid traffic accidents between human and mobile robot should be investigated. Among variety of research, most research focused on the collision avoidance with the obstacles and few research focused on pedestrians who walk following the traffic rules. If the mobile robot is trained how pedestrians walk under the traffic rule, we can achieve safer traffic for each other. This paper develops a new intelligent path planning algorithm to involve intelligence related to pedestrian traffic rules. We employ deep reinforcement learning for realizing the purpose. We use actual pedestrian walking trajectory data. The mobile robot agent learns pedestrian traffic rules by giving positive rewards for behaviors that conform to the pedestrian traffic rules and negative rewards for behaviors that do not conform. The proposed algorithm outputs translational speed and angular speed, considering the pedestrian’s relative positions, forwarding direction, and speed at the closest distance from the mobile robot. Based on learning results in the simulation environment, we confirmed that the mobile robot run in the manner that it understood the pedestrian traffic rules.","","978-4-9077-6478-4","10.23919/SICE56594.2022.9905845","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9905845","mobile robot;deep reinforcement learning;pedestrian traffic rules","Legged locomotion;Navigation;Instruments;Urban areas;Reinforcement learning;Probabilistic logic;Behavioral sciences","collision avoidance;deep learning (artificial intelligence);mobile robots;path planning;reinforcement learning;road traffic control","pedestrian traffic rules;deep reinforcement learning;obstacles;collision avoidance;intelligent mobile robot path planning algorithm;mobile robot run;traffic rule;traffic accidents","","","","6","","6 Oct 2022","","","IEEE","IEEE Conferences"
"Study on Deep Reinforcement Learning for Mobile Robots Flocking Control in Certainty Situations","P. Kheawkhem; I. Khuankrue","Dept. of Control System and Instrumentation Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, Thailand; Dept. of Control System and Instrumentation Engineering, King Mongkut’s University of Technology Thonburi, Bangkok, Thailand","2022 19th International Conference on Electrical Engineering/Electronics, Computer, Telecommunications and Information Technology (ECTI-CON)","16 Jun 2022","2022","","","1","4","Mobile robots are the widely used machines that will be a part of life. It can easily be scaled to meet the requirements of human uses and can move to different target areas freely. However, mobile robots have a problem with mobility, which needs to respond to human activities and obstacles. Reinforcement learning (RT) is a part of machine learning, which enables the machine to learn by themselves. It can develop in a real-world environment without machine teaching or patterns. This paper proposed the study on the flocking control simulation, which avoids the obstacles. Mobile robots in simulation presented by using Multi-Agent Deep Deterministic Policy Gradient (MAD-DPG). The proposed algorithm, the deep reinforcement learning algorithm, is the main navigation of robots, which find the target, maintain distance between robots, and avoid collision with obstacles by learning features and characteristics with certainty environment, obstacle, robots, and target. On the simulation, the authors operate all of the positions in the environment, and robots speed in the environment as the state.","","978-1-6654-8584-5","10.1109/ECTI-CON54298.2022.9795641","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9795641","mobile robots;flocking control;multi-agent deep deterministic policy gradient;simulation;reinforcement learning","Navigation;Computational modeling;Education;Reinforcement learning;Telecommunications;Mobile robots;Collision avoidance","collision avoidance;control engineering computing;deep learning (artificial intelligence);mobile robots;multi-agent systems;multi-robot systems;reinforcement learning","mobile robots flocking control;machine learning;multiagent deep deterministic policy gradient;deep reinforcement learning;MAD-DPG","","1","","13","IEEE","16 Jun 2022","","","IEEE","IEEE Conferences"
"Autonomous UAV Navigation via Deep Reinforcement Learning Using PPO","B. Kabas","Elektrik-Elektronik Mühendisliği Bölümü, Abdullah Gül Üniversitesi, Kayseri, Türkiye","2022 30th Signal Processing and Communications Applications Conference (SIU)","29 Aug 2022","2022","","","1","4","In this paper, a computer vision-based navigation system is proposed for autonomous unmanned aerial vehicles (UAV). The proposed navigation system is based on a deep reinforcement learning-based high-level controller. In this paper, proximal policy optimization (PPO), which is a deep reinforcement learning method, is used to train the artificial neural net-work in an end-to-end way using a continuous reward function. The proposed method has been tested on images obtained from different modalities (RGB and depth) in simulation environments that are created using Unreal Engine and Microsoft AirSim. For the navigation problem that this work is concerned with, a success rate of 96% has been obtained by using RGB cameras. Since RGB cameras are lighter than depth cameras and the trained artificial neural network has a parameter number less than 170.000, the proposed method is suitable to be deployed in micro aerial vehicles. Code is publicly available*.","2165-0608","978-1-6654-5092-8","10.1109/SIU55565.2022.9864769","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864769","deep reinforcement learning;autonomous navigation","Codes;Navigation;Atmospheric modeling;Reinforcement learning;Artificial neural networks;Signal processing;Cameras","aerospace simulation;autonomous aerial vehicles;deep learning (artificial intelligence);image colour analysis;mobile robots;navigation;optimisation;path planning;reinforcement learning;robot vision","Unreal Engine;Microsoft AirSim;depth cameras;RGB cameras;continuous reward function;artificial neural net-work;proximal policy optimization;deep reinforcement learning-based high-level controller;autonomous unmanned aerial vehicles;computer vision-based navigation system;PPO;autonomous UAV navigation;microaerial vehicles","","","","0","IEEE","29 Aug 2022","","","IEEE","IEEE Conferences"
"An Improvement on Mapless Navigation with Deep Reinforcement Learning: A Reward Shaping Approach","A. Alipanah; S. A. A. Moosavian","Faculty of Mechanical Engineering, K. N. Toosi University of Technology, Tehran, Iran; Faculty of Mechanical Engineering, K. N. Toosi University of Technology, Tehran, Iran","2022 10th RSI International Conference on Robotics and Mechatronics (ICRoM)","1 Feb 2023","2022","","","261","266","This paper presents an algorithm for mapless motion planning with deep reinforcement learning. Focusing mainly on reward shaping, possible reward functions for the navigation problem are investigated with the deep deterministic policy gradient method. Accordingly, a new reward function to improve the robot's performance is proposed. The input vector consists of 20 distance laser data, the relative goal position, and the last step velocity, while the output is the velocity command. The proposed method improves previous similar algorithms and shows an acceptable performance in a dynamic environment. The results reveal that the new reward function's effect on the algorithm generates smoother moves of the robot, which results in a 40 percent time reduction and a 30 percent total error reduction compared to the original method. Besides, the learning is completed much faster and takes only 5 hours, while the original method takes 16 hours. All the training and testing processes are conducted on a standard personal laptop.","2572-6889","978-1-6654-5452-0","10.1109/ICRoM57054.2022.10025241","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10025241","Motion planning;Navigation;Mobile robots;Collision avoidance;Reinforcement learning","Deep learning;Training;Portable computers;Navigation;Heuristic algorithms;Reinforcement learning;Task analysis","deep learning (artificial intelligence);gradient methods;mobile robots;navigation;path planning;reinforcement learning","deep deterministic policy gradient method;deep reinforcement learning;distance laser data;error reduction;mapless motion planning;mapless navigation;mobile robot navigation problem;relative goal position;reward function;reward shaping approach;step velocity;time 16.0 hour;time 5.0 hour;time reduction;velocity command","","","","17","IEEE","1 Feb 2023","","","IEEE","IEEE Conferences"
"Development of a Web-Based Education System for Deep Reinforcement Learning-Based Autonomous Mobile Robot Navigation in Real World","R. Suenaga; K. Morioka","Institute for Advanced Study of Mathematical Sciences, Meiji University, Tokyo, Japan; Institute for Advanced Study of Mathematical Sciences, Meiji University, Tokyo, Japan","2020 IEEE/SICE International Symposium on System Integration (SII)","9 Mar 2020","2020","","","1040","1045","The technology that combined deep reinforcement learning and robotics is increasing interests in recent years. Although several online tools for studying this technology can be found, it is difficult for beginners to develop actual robot systems for autonomous navigation in the real world. In this study, we developed a web-based educational system that is able to help users to study mobile robot navigation based on deep reinforcement learning and develop actual robot systems. The proposed web system provides the following functions: setting the parameters of reinforcement learning for autonomous robot navigation, running learning scripts and monitoring status of the learning. The first experiment that a user develops an actual robot system was performed. In the experiment, the user tuned parameters on the web page started the training and obtained action policy models. The experimental results indicate the proposed system can be applied to develop an actual autonomous navigation system. Also, the user could decide better parameters through the trial and error process using the proposed system.","2474-2325","978-1-7281-6667-4","10.1109/SII46433.2020.9025980","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9025980","","Navigation;Mobile robots;Machine learning;Training;Robot sensing systems;Servers","control engineering education;educational robots;Internet;learning (artificial intelligence);mobile robots;navigation;neural nets;path planning","robotics;learning scripts;deep reinforcement learning;autonomous mobile robot navigation;Web-based educational system","","","","10","IEEE","9 Mar 2020","","","IEEE","IEEE Conferences"
"Autonomous Robot Navigation in Dynamic Environment Using Deep Reinforcement Learning","X. Qiu; K. Wan; F. Li","China Aeronautical Radio Electronics, Research Institute, Science and Technology on Avionics Integration Laboratory, Shanghai, China; Northwestern Polytechnical University, School of Electronics and Information, Xi'an, China; Northwestern Polytechnical University, School of Electronics and Information, Xi'an, China","2019 IEEE 2nd International Conference on Automation, Electronics and Electrical Engineering (AUTEEE)","12 Mar 2020","2019","","","338","342","Compared to traditional control methods, deep reinforcement learning (DRL) has the ability to learn how to solve complex tasks in a dynamic environment simply by collecting experience. In this paper, we study the application of DRL method in robotic autonomous control with detection capability in simulated dynamic environment. More specifically, we have adopted Deep Q Network (DQN), double DQN and dueling DQN algorithms in DRL. As with fixed reward settings, these original DRL algorithms do not perform well while navigating a robot in dynamic environment. To address the problems, we designed a novel reward shaping method and conducted a series of experiment with all three improved DRL algorithms. The results show that the new reward shaping method can significantly improve the DRL performance when they are applied in robot navigation settings.","","978-1-7281-5030-7","10.1109/AUTEEE48671.2019.9033166","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9033166","robot navigation;deep reinforcement learning;deep Q network;reward shaping","Navigation;Heuristic algorithms;Machine learning;Robot sensing systems;Neural networks;Collision avoidance","learning (artificial intelligence);mobile robots;path planning","autonomous robot navigation;deep reinforcement learning;traditional control methods;DRL method;robotic autonomous control;simulated dynamic environment;deep Q network;double DQN;dueling DQN algorithms;fixed reward settings;original DRL algorithms;reward shaping method;improved DRL algorithms;DRL performance;robot navigation settings","","3","","21","IEEE","12 Mar 2020","","","IEEE","IEEE Conferences"
"Path planning of mobile robot based on deep reinforcement learning with transfer learning strategy","J. Zhu; C. Yang; Z. Liu; C. Yang","School of Automation and Electrical Engineering, Linyi university, Linyi, China; School of Information Science and Engineering, Linyi university, Linyi, China; School of Automation and Electrical Engineering, Linyi university, Linyi, China; School of Information Science and Engineering, Linyi university, Linyi, China","2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC)","30 Jan 2023","2022","","","1242","1246","Under complex environments, mobile robots can decision-making, autonomous learning, intelligent obstacle avoidance, and complete the task from start point to endpoint. This paper designed the mobile robot, excluding planners and unknown maps, which can successfully reach the target by autonomously learning and navigating in the unknown environment. By applying deep reinforcement learning to the path planning of mobile robots, the robot can collect data and conduct training on its own, and improve it autonomously without manual supervision. Consequently, it can complete the path planning task. The application of transfer learning improves the adaptive efficiency of the mobile robot to the environment. Finally, the results are verified by comparative experiments in three simulation environments.","","978-1-6654-6536-6","10.1109/YAC57282.2022.10023708","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10023708","deep reinforcement learning;mobile robots;obstacle avoidance","Training;Deep learning;Navigation;Transfer learning;Supervised learning;Reinforcement learning;Path planning","collision avoidance;learning (artificial intelligence);mobile robots;path planning","autonomous learning;deep reinforcement learning;mobile robot;path planning task;transfer learning strategy","","","","21","IEEE","30 Jan 2023","","","IEEE","IEEE Conferences"
